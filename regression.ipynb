{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054f247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92aaaa",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74061bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 34594\n",
      "Test Size: 8683\n"
     ]
    }
   ],
   "source": [
    "# Prepare the train / test file\n",
    "starts = [1 + 5000 * i for i in range(10)]  # train-test: 4-1\n",
    "\n",
    "target_subs = []\n",
    "feature_subs = []\n",
    "\n",
    "for start in starts:\n",
    "    end = start + 5000 - 1\n",
    "    folder = f\"outputs/{start}_{end}\"\n",
    "\n",
    "    # Define data paths\n",
    "    target_file = os.path.join(folder, \"kpi\", \"kpi.csv\")\n",
    "    feature_files = [\n",
    "        os.path.join(folder, \"metadata\", \"metadata.csv\"),\n",
    "        os.path.join(folder, \"metadata\", \"calibration.csv\"),\n",
    "        os.path.join(folder, \"metadata\", \"ego_motion.csv\"),\n",
    "        os.path.join(folder, \"metadata\", \"image_features.csv\"),\n",
    "    ]\n",
    "\n",
    "    # Read data\n",
    "    target_df = pd.read_csv(target_file)\n",
    "    feature_df = pd.concat(\n",
    "        [pd.read_csv(fp).drop(\"image_id\", axis=1) for fp in feature_files], axis=1\n",
    "    )\n",
    "\n",
    "    # Append to list\n",
    "    target_subs.append(target_df)\n",
    "    feature_subs.append(feature_df)\n",
    "\n",
    "# Merge as train / test data\n",
    "target_data_train = pd.concat(target_subs[:8], axis=0)\n",
    "feature_data_train = pd.concat(feature_subs[:8], axis=0)\n",
    "\n",
    "data_train = pd.concat([target_data_train, feature_data_train], axis=1)\n",
    "print(f\"Train Size: {len(data_train)}\")\n",
    "\n",
    "target_data_test = pd.concat(target_subs[8:], axis=0)\n",
    "feature_data_test = pd.concat(feature_subs[8:], axis=0)\n",
    "\n",
    "data_test = pd.concat([target_data_test, feature_data_test], axis=1)\n",
    "print(f\"Test Size: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1677a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to train / test file\n",
    "data_train.to_csv(\"classification_train.csv\", index=False)\n",
    "data_test.to_csv(\"classification_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287158fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_filename: str, test_filename: str, target: str, features: list) -> dict:\n",
    "    df_train = pd.read_csv(train_filename)\n",
    "    df_test = pd.read_csv(test_filename)\n",
    "\n",
    "    data = {\n",
    "        'train': {\n",
    "            'features': df_train[features].to_numpy(),\n",
    "            'target': df_train[target].to_numpy()\n",
    "        },\n",
    "        'test': {\n",
    "            'features': df_test[features].to_numpy(),\n",
    "            'target': df_test[target].to_numpy()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba562357",
   "metadata": {},
   "source": [
    "#### Pre-Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a0317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_krr() -> tuple:\n",
    "    # Define the KernalRidge model\n",
    "    model = KernelRidge()\n",
    "\n",
    "    # Difine the parameter grids\n",
    "    param_grid = {\n",
    "        \"kernel\": [\"poly\"],\n",
    "        \"alpha\": [0.1],\n",
    "        \"gamma\": [0.01],\n",
    "        \"degree\": [3, 4],\n",
    "    }\n",
    "    return model, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5308b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gridsearch(model, param_grid: dict) -> GridSearchCV:\n",
    "    return GridSearchCV(model, param_grid, cv=8, scoring='neg_mean_squared_error', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b595d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_pipeline() -> tuple:\n",
    "    # Define a Standard Scaler to normalize inputs\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"num\", StandardScaler(), list(range(2, 8))),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), list(range(0, 2))),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define PCA to reduce feature\n",
    "    pca = PCA()\n",
    "\n",
    "    # Define the KernalRidge model\n",
    "    krr = KernelRidge(kernel=\"poly\")\n",
    "\n",
    "    # Difine the parameter grids\n",
    "    param_grid = {\n",
    "        \"pca__n_components\": [2, 4, 6],\n",
    "        \"krr__alpha\": [0.1, 0.01],\n",
    "        \"krr__gamma\": [0.01, 0.01],\n",
    "        \"krr__degree\": [3, 4],\n",
    "    }\n",
    "\n",
    "    # Assemble the pipeline\n",
    "    pipe = Pipeline(steps=[(\"pre\", preprocessor), (\"pca\", pca), (\"krr\", krr)])\n",
    "\n",
    "    return pipe, param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2bed8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29cbb076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyper_model: GridSearchCV, data: dict, name: str, features: list) -> BaseEstimator:\n",
    "    # Train the model\n",
    "    hyper_model.fit(data['train']['features'], data[\"train\"]['target'])\n",
    "\n",
    "    # Use the trained model to predict\n",
    "    y_pred_test = hyper_model.predict(data['test']['features'])\n",
    "    y_pred_train = hyper_model.predict(data['train']['features'])\n",
    "\n",
    "    # Assess the model quality\n",
    "    r2_test = r2_score(data[\"test\"]['target'], y_pred_test)\n",
    "    mae_test = mean_absolute_error(data[\"test\"]['target'], y_pred_test)\n",
    "    mse_test = mean_squared_error(data[\"test\"]['target'], y_pred_test)\n",
    "\n",
    "    r2_train = r2_score(data[\"train\"]['target'], y_pred_train)\n",
    "    mae_train = mean_absolute_error(data[\"train\"]['target'], y_pred_train)\n",
    "    mse_train = mean_squared_error(data[\"train\"]['target'], y_pred_train)\n",
    "    \n",
    "    print('------- Test -------')\n",
    "    print(f'r2: {r2_test:.3f} | mean absolote error: {mae_test:.3f} | mean squared error: {mse_test:.3f}')\n",
    "    \n",
    "    print('------- Train -------')\n",
    "    print(f'r2: {r2_train:.3f} | mean absolote error: {mae_train:.3f} | mean squared error: {mse_train:.3f}')\n",
    "\n",
    "    # Record the parameters if r2_test > 0.9\n",
    "   \n",
    "    best_params = hyper_model.best_params_\n",
    "    print('------- Best Params -------')\n",
    "    print(best_params)\n",
    "\n",
    "    try:\n",
    "        json_str = {\n",
    "        \"best_params\": {\n",
    "            \"alpha\": best_params['alpha'],\n",
    "            \"gamma\": best_params['gamma'],\n",
    "            \"kernel\": \"polynomial\"\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"mae\": mae_train,\n",
    "            \"mse\": mse_train,\n",
    "            \"r2\": r2_train\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"mae\": mae_test,\n",
    "            \"mse\": mse_test,\n",
    "            \"r2\": r2_test\n",
    "        },\n",
    "        \"feature\": features\n",
    "    } \n",
    "    except:\n",
    "        json_str = {\n",
    "        \"best_params\": {\n",
    "            \"alpha\": best_params['krr__alpha'],\n",
    "            \"gamma\": best_params['krr__gamma'],\n",
    "            \"kernel\": \"polynomial\"\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"mae\": mae_train,\n",
    "            \"mse\": mse_train,\n",
    "            \"r2\": r2_train\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"mae\": mae_test,\n",
    "            \"mse\": mse_test,\n",
    "            \"r2\": r2_test\n",
    "        },\n",
    "        \"feature\": features\n",
    "    } \n",
    "    \n",
    "    with open(f\"{name}.json\", mode='w') as f:\n",
    "        json.dump(json_str, f, indent=2)\n",
    "    \n",
    "    with open(f\"{name}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(hyper_model, f)\n",
    "\n",
    "    return hyper_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c329826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Test -------\n",
      "r2: 0.008 | mean absolote error: 0.149 | mean squared error: 0.034\n",
      "------- Train -------\n",
      "r2: 0.024 | mean absolote error: 0.151 | mean squared error: 0.034\n",
      "------- Best Params -------\n",
      "{'krr__alpha': 0.1, 'krr__degree': 3, 'krr__gamma': 0.01, 'pca__n_components': 6}\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "features = [\n",
    "    # \"country_code\",\n",
    "    \"weather\",\n",
    "    # \"road_type\",\n",
    "    # \"road_condition\",\n",
    "    \"time_of_day\",\n",
    "    # \"num_vehicles\",\n",
    "    # \"longitude\",\n",
    "    # \"latitude\",\n",
    "    \"solar_angle_elevation\",\n",
    "    # \"focal_length_x\",\n",
    "    # \"focal_length_y\",\n",
    "    # \"principle_point_x\",\n",
    "    # \"principle_point_y\",\n",
    "    # \"camera_pose_x\",\n",
    "    # \"camera_pose_y\",\n",
    "    # \"camera_pose_z\",\n",
    "    # \"camera_pose_yaw\",\n",
    "    # \"camera_pose_pitch\",\n",
    "    # \"camera_pose_roll\",\n",
    "    # \"horizontal_fov\",\n",
    "    # \"vertical_fov\",\n",
    "    # \"ego_pose_x\",\n",
    "    # \"ego_pose_y\",\n",
    "    # \"ego_pose_z\",\n",
    "    # \"ego_pose_yaw\",\n",
    "    # \"ego_pose_pitch\",\n",
    "    # \"ego_pose_roll\",\n",
    "    # \"speed_var\",\n",
    "    # \"mean_jerk\",\n",
    "    # \"max_jerk\",\n",
    "    # \"st_jerk\",\n",
    "    # \"mean_angular_acc\",\n",
    "    # \"max_angular_acc\",\n",
    "    # \"st_angular_acc\",\n",
    "    # \"mean_lateral_acc\",\n",
    "    # \"max_lateral_acc\",\n",
    "    # \"st_lateral_acc\",\n",
    "    \"luminance\",\n",
    "    \"contrast\",\n",
    "    \"saturation\",\n",
    "    \"sharpness\",\n",
    "    \"temperature\",\n",
    "    \"edge_density\", \n",
    "    \"entropy\"\n",
    "]\n",
    "\n",
    "# Perform training\n",
    "data = get_data(\"regression_train.csv\", \"regression_test.csv\", \"accuracy\", features)\n",
    "\n",
    "pca_pipeline, param_grid = get_pca_pipeline()\n",
    "# model, param_grid = get_krr()\n",
    "hyper_model = get_gridsearch(pca_pipeline, param_grid)\n",
    "best_model = train(hyper_model, data, \"krr_polynomial\", features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
