{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the predicted annotations and format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 45001\n",
    "end = start + 5000 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fp = f\"predictions/frames_zod_detections_{start}_{end}.json\"\n",
    "with open(pred_fp, \"r\") as file:\n",
    "    pred_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create containers for attributes\n",
    "img_id = []\n",
    "x1_ls = []\n",
    "y1_ls = []\n",
    "x2_ls = []\n",
    "y2_ls = []\n",
    "class_ls = []\n",
    "conf_ls = []\n",
    "area_ls = []\n",
    "\n",
    "# Loop through the data and collect attribute values\n",
    "for key, value in pred_data.items():\n",
    "    for i in value:\n",
    "        x1, y1, x2, y2 = i[\"box\"]\n",
    "        cls = \"Vehicle\"\n",
    "        conf = i[\"confidence\"]\n",
    "        area = i[\"area\"]\n",
    "\n",
    "        # Append to the corresponding list\n",
    "        img_id.append(key.split(\"_\")[0])\n",
    "        x1_ls.append(x1)\n",
    "        y1_ls.append(y1)\n",
    "        x2_ls.append(x2)\n",
    "        y2_ls.append(y2) \n",
    "        class_ls.append(cls)\n",
    "        conf_ls.append(conf)\n",
    "        area_ls.append(area)\n",
    "\n",
    "# Rearrange the data into dictionary\n",
    "new_pred_data = {\n",
    "    \"image_id\": img_id,\n",
    "    \"x1\": x1_ls,\n",
    "    \"y1\": y1_ls,\n",
    "    \"x2\": x2_ls,\n",
    "    \"y2\": y2_ls,\n",
    "    \"class\": class_ls,\n",
    "    \"area\": area_ls,\n",
    "    \"confidence\": conf_ls,\n",
    "}\n",
    "\n",
    "# Wrap the new data as a pandas dataframe\n",
    "pred_df = pd.DataFrame(new_pred_data)\n",
    "pred_df = pred_df.sort_values(\"image_id\")\n",
    "\n",
    "pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to csv\n",
    "pred_df.to_csv(f\"outputs/{start}_{end}/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the ground-truth annotations and format the data as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read groud truth data\n",
    "img_ids = pred_df[\"image_id\"].unique()\n",
    "true_data = {}\n",
    "for img_id in img_ids:\n",
    "    fp = f\"single_frames/{img_id}/annotations/object_detection.json\"\n",
    "    with open(fp, \"r\") as file:\n",
    "        true_data[img_id] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth attributes needed: coordinates, class, area and occolusion ratio\n",
    "# Create container for attributes\n",
    "img_id_ls = []\n",
    "x1_ls = []\n",
    "y1_ls = []\n",
    "x2_ls = []\n",
    "y2_ls = []\n",
    "class_ls = []\n",
    "area_ls = []\n",
    "oc_ratio_ls = []\n",
    "\n",
    "for img_id, img_annots in true_data.items():\n",
    "    for obj in img_annots:\n",
    "        px = [p[0] for p in obj[\"geometry\"][\"coordinates\"]] # Coordinates\n",
    "        py = [p[1] for p in obj[\"geometry\"][\"coordinates\"]]\n",
    "        x1, y1, x2, y2 = int(min(px)), int(min(py)), int(max(px)), int(max(py))\n",
    "            \n",
    "        klass = obj[\"properties\"][\"class\"]  # Class\n",
    "        \n",
    "        area = abs((x1 - x2) * (y1 - y2))\n",
    "\n",
    "        try:\n",
    "            oc_ratio = obj[\"properties\"][\"occlusion_ratio\"] # Occolusion ratio\n",
    "        except:\n",
    "            oc_ratio = \"Undefined\"\n",
    "\n",
    "        # Append to the corresponding list\n",
    "        img_id_ls.append(img_id) \n",
    "        x1_ls.append(x1)\n",
    "        y1_ls.append(y1)\n",
    "        x2_ls.append(x2)\n",
    "        y2_ls.append(y2)\n",
    "        class_ls.append(klass)\n",
    "        area_ls.append(area)\n",
    "        oc_ratio_ls.append(oc_ratio)\n",
    "\n",
    "    # Rearrange the data into dictionary\n",
    "    new_true_data = {\n",
    "        \"image_id\": img_id_ls,\n",
    "        \"x1\": x1_ls,\n",
    "        \"y1\": y1_ls,\n",
    "        \"x2\": x2_ls,\n",
    "        \"y2\": y2_ls,\n",
    "        \"class\": class_ls,\n",
    "        \"area\": area_ls,\n",
    "        \"occolusion_ratio\": oc_ratio_ls\n",
    "    }\n",
    "# Wrap the new data as a pandas dataframe\n",
    "true_df = pd.DataFrame(new_true_data)\n",
    "\n",
    "true_df[true_df[\"class\"]==\"Vehicle\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to csv\n",
    "true_df.to_csv(f\"outputs/{start}_{end}/ground_truths.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding Box Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bboxes(image_path, pred_bboxes, gt_bboxes):\n",
    "    \"\"\"\n",
    "    Visualize prediction and ground truth bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        pred_bboxes (list of tuples): List of predicted bounding boxes [(x1, y1, x2, y2), ...].\n",
    "        gt_bboxes (list of tuples): List of ground truth bounding boxes [(x1, y1, x2, y2), ...].\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw prediction boxes in blue\n",
    "    for (x1, y1, x2, y2) in pred_bboxes:\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Blue for predictions\n",
    "        cv2.putText(image, 'Pred', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    \n",
    "    # Draw ground truth boxes in green\n",
    "    for (x1, y1, x2, y2) in gt_bboxes:\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green for ground truth\n",
    "        cv2.putText(image, 'GT', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check the range of predicted vehicle area\n",
    "Decide whether a filtering is needed for groud truth objects\n",
    "'''\n",
    "start_ls = [i*5000+1 for i in range(10)]\n",
    "all_areas_pred = []\n",
    "\n",
    "for start in start_ls:\n",
    "    end = start + 5000 - 1\n",
    "    pred_fp = f\"outputs/{start}_{end}/predictions.csv\"\n",
    "    pred_df = pd.read_csv(pred_fp)\n",
    "    all_areas_pred.append(pred_df[\"area\"])\n",
    "\n",
    "all_areas_pred = pd.concat(all_areas_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_areas_pred.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log1p(all_areas_pred), bins=20, edgecolor=\"black\")\n",
    "plt.xlabel(\"log(1 + Area)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram with Log Transformation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If 400 is used as the filtering criteria, check how many ground truth objects will be removed (count: 1398989).\n",
    "In addition, check how many vehicles will be removed (count: 124939). \n",
    "'''\n",
    "start_ls = [i*5000+1 for i in range(10)]\n",
    "all_areas_pred = []\n",
    "\n",
    "for start in start_ls:\n",
    "    end = start + 5000 - 1\n",
    "    pred_fp = f\"outputs/{start}_{end}/ground_truths.csv\"\n",
    "    pred_df = pd.read_csv(pred_fp)\n",
    "    pred_df = pred_df[pred_df[\"class\"]==\"Vehicle\"]\n",
    "    all_areas_pred.append(pred_df[\"area\"])\n",
    "\n",
    "all_areas_pred = pd.concat(all_areas_pred)\n",
    "\n",
    "threshold = 400\n",
    "count = (all_areas_pred < threshold).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do removal\n",
    "start_ls = [i*5000+1 for i in range(10)]\n",
    "\n",
    "for start in start_ls:\n",
    "    end = start + 5000 - 1\n",
    "    pred_fp = f\"outputs/{start}_{end}/ground_truths.csv\"\n",
    "    pred_df = pd.read_csv(pred_fp)\n",
    "    selector = pred_df[\"area\"] > 400\n",
    "    pred_df = pred_df[selector]\n",
    "    pred_df.to_csv(f\"outputs/{start}_{end}/ground_truths_less.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate KPIs (1) - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    box1 = list(box1)\n",
    "    box2 = list(box2)\n",
    "    # Coordinates of intersection rectangle\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # Compute area of intersection\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # Compute areas of both bounding boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Union area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # IoU calculation\n",
    "    iou = intersection_area / union_area if union_area != 0 else 0\n",
    "    return round(iou, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "print(calculate_iou([0, 0, 2, 2], [-0.5, -0.5, 1.5, 1.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate iou for the selected 5000 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results = {\n",
    "    \"image_id\": [],\n",
    "    \"pred_obj\": [],\n",
    "    \"true_obj\": [],\n",
    "    \"iou\": [],\n",
    "    \"occolusion_ratio\": [] \n",
    "}\n",
    "for img_id in pred_df[\"image_id\"].unique():\n",
    "    preds = pred_df[pred_df[\"image_id\"] == img_id]\n",
    "    truths = true_df[true_df[\"image_id\"] == img_id]\n",
    "\n",
    "    for pred_idx, pred_obj in preds.iterrows():\n",
    "        # Maintain a list of IoUs\n",
    "        iou_ls = []\n",
    "\n",
    "        # Calculate IoU with all ground truth bounding boxes\n",
    "        for _, true_obj in truths.iterrows():\n",
    "            iou = calculate_iou(pred_obj.iloc[1:5], true_obj.iloc[1:5])\n",
    "            iou_ls.append(iou)\n",
    "        \n",
    "        true_idx = np.argmax(iou_ls)\n",
    "\n",
    "        # Save the result\n",
    "        iou_results[\"image_id\"].append(img_id)\n",
    "        iou_results[\"pred_obj\"].append(pred_idx)\n",
    "        iou_results[\"true_obj\"].append(true_idx)\n",
    "        iou_results[\"iou\"].append(max(iou_ls))\n",
    "        iou_results[\"occolusion_ratio\"].append(truths.iloc[true_idx, -1])\n",
    "\n",
    "iou_results_df = pd.DataFrame(iou_results)\n",
    "iou_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to csv\n",
    "iou_results_df.to_csv(f\"outputs/{start}_{end}/kpi/iou.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics for the iou results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some summary statistics\n",
    "print(\"Summary by thresholds: \")\n",
    "for threshold in np.arange(0, 1, 0.1):\n",
    "    selector = iou_results_df[\"iou\"] >= threshold\n",
    "    count = len(iou_results_df[selector])\n",
    "\n",
    "    print(f\"predicted obj with iou >= {threshold:.1f}: {count}\")\n",
    "\n",
    "print(\"\\nSummary by occolusion level:\")\n",
    "for oratio in iou_results_df[\"occolusion_ratio\"].unique():\n",
    "    selector = (iou_results_df[\"occolusion_ratio\"] == oratio)\n",
    "    mean = np.mean(iou_results_df[selector][\"iou\"])\n",
    "\n",
    "    print(f\"mean iou for occlusion ratio {oratio}: {mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results by visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = \"012120\"\n",
    "\n",
    "# Search for the image path\n",
    "directory = f\"single_frames_img/{img_id}/camera_front_blur\"\n",
    "img_path = os.path.join(directory, os.listdir(directory)[0])\n",
    "\n",
    "selector1 = pred_df[\"image_id\"]==img_id\n",
    "pred_bboxes = pred_df[selector1].iloc[:, 1:5].to_numpy()\n",
    "\n",
    "selector2 = (true_df[\"image_id\"]==img_id) & (true_df[\"class\"]==\"Vehicle\")\n",
    "gt_bboxes = true_df[selector2].iloc[:, 1:5].to_numpy()\n",
    "\n",
    "bbox_image = visualize_bboxes(img_path, pred_bboxes, gt_bboxes)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(bbox_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate KPIs (2) - TP, FP, FN, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to calculate TP\n",
    "# TP: prediction = car, truth = Vehicle\n",
    "def cal_kpi(preds: pd.DataFrame, truths: pd.DataFrame, threshold):\n",
    "    ''' \n",
    "    This function shall be performed for each image.\n",
    "    The input preds and truths contains the predicted and the ground-truth objects in an image.\n",
    "    '''\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    if_true_detected = np.zeros(len(truths))   # Maintain a list tracking whether a ground-truth object is detected\n",
    "    \n",
    "    # Loop through all predicted objects\n",
    "    for _, pred_obj in preds.iterrows():\n",
    "        # Maintain a list of IoUs\n",
    "        iou_ls = []\n",
    "        \n",
    "        # Calculate IoU with all ground truth bounding boxes\n",
    "        for _, true_obj in truths.iterrows():\n",
    "            iou = calculate_iou(pred_obj[1:5], true_obj[1:5])\n",
    "            iou_ls.append(iou)\n",
    "        \n",
    "        # Find the best match\n",
    "        idx = np.argmax(iou_ls) # This one is the index of the iou maxima and groud truth\n",
    "        iou_max = iou_ls[idx]\n",
    "\n",
    "        # Compare with the threshold\n",
    "        if iou_max >= threshold:\n",
    "            # Correct detection\n",
    "            if truths[\"class\"].iloc[idx] == \"Vehicle\":\n",
    "                # Correct classification => TP\n",
    "                tp += 1\n",
    "            else:\n",
    "                # Incorrect classification => FP (for the vechile class)\n",
    "                fp += 1\n",
    "            # Update the if_true_detected\n",
    "            if_true_detected[idx] += 1\n",
    "        else:\n",
    "            # Incorrect detection of a vehicle => FP\n",
    "            fp +=1\n",
    "        \n",
    "    # Calculate FN: Vehicles in the ground truth not detected\n",
    "    selector1 = (if_true_detected == 0) # Select objects in the ground truth not detected\n",
    "    obj_not_detected = truths[selector1]\n",
    "    selector2 = (obj_not_detected[\"class\"] == \"Vehicle\") # Select undetected ground truth objects that are vehicle => FN\n",
    "    car_not_detected = obj_not_detected[selector2]\n",
    "    fn = len(car_not_detected)\n",
    "    \n",
    "    # # Debugging: check the correctness of tp, fp, fn counts\n",
    "    # # Rule 1: tp + fp = the number of predicted car\n",
    "    # print(f\"if tp + fp = the number of predicted car: {tp + fp == len(preds)}\")\n",
    "    # # Rule 2: tp + fn = the number of ground truth car\n",
    "    # print(f\"if tp + fp = the number of ground truth car: {tp + fp == len(truths[truths[\"class\"] == \"Vehicle\"])}\")\n",
    "\n",
    "    # Calculate precision, recall, and accurary\n",
    "    epsilon = 2E-5\n",
    "    precision = round(tp / (tp + fp + epsilon), 3)\n",
    "    recall = round(tp / (tp + fn + epsilon), 3)\n",
    "    \n",
    "    return tp, fp, fn, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all img_ids\n",
    "img_ids = pred_df[\"image_id\"].unique()\n",
    "col_names = [\"image_id\", \"tp\", \"fp\", \"fn\", \"precision\", \"recall\"]\n",
    "rows = [] \n",
    "\n",
    "# Iterate through each img\n",
    "for img_id in img_ids:\n",
    "    preds_img = pred_df[pred_df[\"image_id\"]==img_id]\n",
    "    truths_img = true_df[true_df[\"image_id\"]==img_id]\n",
    "    tp, fp, fn, precision, recall = cal_kpi(preds_img, truths_img, threshold=0.75)\n",
    "    # print(f\"id: {img_id}, tp: {tp} fp: {fp}, fn: {fn}, precision: {precision}, recall: {recall}\")\n",
    "    rows.append([img_id, tp, fp, fn, precision, recall])\n",
    "\n",
    "kpi_df = pd.DataFrame(rows, columns=col_names)\n",
    "\n",
    "kpi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "kpi_df.to_csv(f\"outputs/{start}_{end}/kpi/kpi_threshold_0_75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = (kpi_df[\"precision\"] >= 0.5)\n",
    "print(f\"{len(kpi_df[selector])} / {len(kpi_df)}\")\n",
    "\n",
    "selector = (kpi_df[\"recall\"] >= 0.5)\n",
    "print(f\"{len(kpi_df[selector])} / {len(kpi_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. Have a summary of kpis (mean)\n",
    "2. Add accuracy\n",
    "3. Check the training data to account for low recall.\n",
    "4. Set a higher priority on closer (larger box or distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
