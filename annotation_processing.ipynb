{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the predicted annotations and format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 45001\n",
    "end = start + 5000 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fp = f\"predictions/frames_zod_detections_{start}_{end}.json\"\n",
    "with open(pred_fp, \"r\") as file:\n",
    "    pred_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create containers for attributes\n",
    "img_id = []\n",
    "x1_ls = []\n",
    "y1_ls = []\n",
    "x2_ls = []\n",
    "y2_ls = []\n",
    "class_ls = []\n",
    "conf_ls = []\n",
    "area_ls = []\n",
    "\n",
    "# Loop through the data and collect attribute values\n",
    "for key, value in pred_data.items():\n",
    "    for i in value:\n",
    "        x1, y1, x2, y2 = i[\"box\"]\n",
    "        cls = \"Vehicle\"\n",
    "        conf = i[\"confidence\"]\n",
    "        area = i[\"area\"]\n",
    "\n",
    "        # Append to the corresponding list\n",
    "        img_id.append(key.split(\"_\")[0])\n",
    "        x1_ls.append(x1)\n",
    "        y1_ls.append(y1)\n",
    "        x2_ls.append(x2)\n",
    "        y2_ls.append(y2) \n",
    "        class_ls.append(cls)\n",
    "        conf_ls.append(conf)\n",
    "        area_ls.append(area)\n",
    "\n",
    "# Rearrange the data into dictionary\n",
    "new_pred_data = {\n",
    "    \"image_id\": img_id,\n",
    "    \"x1\": x1_ls,\n",
    "    \"y1\": y1_ls,\n",
    "    \"x2\": x2_ls,\n",
    "    \"y2\": y2_ls,\n",
    "    \"class\": class_ls,\n",
    "    \"area\": area_ls,\n",
    "    \"confidence\": conf_ls,\n",
    "}\n",
    "\n",
    "# Wrap the new data as a pandas dataframe\n",
    "pred_df = pd.DataFrame(new_pred_data)\n",
    "pred_df = pred_df.sort_values(\"image_id\")\n",
    "\n",
    "pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to csv\n",
    "pred_df.to_csv(f\"outputs/{start}_{end}/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the ground-truth annotations and format the data as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read groud truth data\n",
    "img_ids = pred_df[\"image_id\"].unique()\n",
    "true_data = {}\n",
    "for img_id in img_ids:\n",
    "    fp = f\"single_frames/{img_id}/annotations/object_detection.json\"\n",
    "    with open(fp, \"r\") as file:\n",
    "        true_data[img_id] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth attributes needed: coordinates, class, area and occolusion ratio\n",
    "# Create container for attributes\n",
    "img_id_ls = []\n",
    "x1_ls = []\n",
    "y1_ls = []\n",
    "x2_ls = []\n",
    "y2_ls = []\n",
    "class_ls = []\n",
    "area_ls = []\n",
    "oc_ratio_ls = []\n",
    "\n",
    "for img_id, img_annots in true_data.items():\n",
    "    for obj in img_annots:\n",
    "        px = [p[0] for p in obj[\"geometry\"][\"coordinates\"]] # Coordinates\n",
    "        py = [p[1] for p in obj[\"geometry\"][\"coordinates\"]]\n",
    "        x1, y1, x2, y2 = int(min(px)), int(min(py)), int(max(px)), int(max(py))\n",
    "            \n",
    "        klass = obj[\"properties\"][\"class\"]  # Class\n",
    "        \n",
    "        area = abs((x1 - x2) * (y1 - y2))\n",
    "\n",
    "        try:\n",
    "            oc_ratio = obj[\"properties\"][\"occlusion_ratio\"] # Occolusion ratio\n",
    "        except:\n",
    "            oc_ratio = \"Undefined\"\n",
    "\n",
    "        # Append to the corresponding list\n",
    "        img_id_ls.append(img_id) \n",
    "        x1_ls.append(x1)\n",
    "        y1_ls.append(y1)\n",
    "        x2_ls.append(x2)\n",
    "        y2_ls.append(y2)\n",
    "        class_ls.append(klass)\n",
    "        area_ls.append(area)\n",
    "        oc_ratio_ls.append(oc_ratio)\n",
    "\n",
    "    # Rearrange the data into dictionary\n",
    "    new_true_data = {\n",
    "        \"image_id\": img_id_ls,\n",
    "        \"x1\": x1_ls,\n",
    "        \"y1\": y1_ls,\n",
    "        \"x2\": x2_ls,\n",
    "        \"y2\": y2_ls,\n",
    "        \"class\": class_ls,\n",
    "        \"area\": area_ls,\n",
    "        \"occolusion_ratio\": oc_ratio_ls\n",
    "    }\n",
    "# Wrap the new data as a pandas dataframe\n",
    "true_df = pd.DataFrame(new_true_data)\n",
    "\n",
    "true_df[true_df[\"class\"]==\"Vehicle\"].head()\n",
    "\n",
    "# Filter by area > 400\n",
    "selector = true_df[\"area\"] > 400\n",
    "true_df = true_df[selector]\n",
    "\n",
    "true_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to csv\n",
    "true_df.to_csv(f\"outputs/{start}_{end}/ground_truths.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate KPIs (1) - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    box1 = list(box1)\n",
    "    box2 = list(box2)\n",
    "    # Coordinates of intersection rectangle\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # Compute area of intersection\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # Compute areas of both bounding boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Union area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # IoU calculation\n",
    "    iou = intersection_area / union_area if union_area != 0 else 0\n",
    "    return round(iou, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results = {\n",
    "    \"image_id\": [],\n",
    "    \"pred_obj\": [],\n",
    "    \"true_obj\": [],\n",
    "    \"iou\": [],\n",
    "    \"occolusion_ratio\": [] \n",
    "}\n",
    "for img_id in pred_df[\"image_id\"].unique():\n",
    "    preds = pred_df[pred_df[\"image_id\"] == img_id]\n",
    "    truths = true_df[true_df[\"image_id\"] == img_id]\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        iou_results[\"image_id\"].append(img_id)\n",
    "        iou_results[\"pred_obj\"].append(None)\n",
    "        iou_results[\"true_obj\"].append(None)\n",
    "        iou_results[\"iou\"].append(0)\n",
    "        iou_results[\"occolusion_ratio\"].append(None)\n",
    "\n",
    "        continue\n",
    "\n",
    "    if len(truths) == 0:\n",
    "        for pred_idx, pred_obj in preds.iterrows():\n",
    "            iou_results[\"image_id\"].append(img_id)\n",
    "            iou_results[\"pred_obj\"].append(pred_idx)\n",
    "            iou_results[\"true_obj\"].append(None)\n",
    "            iou_results[\"iou\"].append(0)\n",
    "            iou_results[\"occolusion_ratio\"].append(None)\n",
    "        \n",
    "        continue\n",
    "\n",
    "    for pred_idx, pred_obj in preds.iterrows():\n",
    "        # Maintain a list of IoUs\n",
    "        iou_ls = []\n",
    "\n",
    "        # Calculate IoU with all ground truth bounding boxes\n",
    "        for _, true_obj in truths.iterrows():\n",
    "            iou = calculate_iou(pred_obj.iloc[1:5], true_obj.iloc[1:5])\n",
    "            iou_ls.append(iou)\n",
    "        \n",
    "        \n",
    "        true_idx = np.argmax(iou_ls)\n",
    "\n",
    "        # Save the result\n",
    "        iou_results[\"image_id\"].append(img_id)\n",
    "        iou_results[\"pred_obj\"].append(pred_idx)\n",
    "        iou_results[\"true_obj\"].append(true_idx)\n",
    "        iou_results[\"iou\"].append(max(iou_ls))\n",
    "        iou_results[\"occolusion_ratio\"].append(truths.iloc[true_idx, -1])\n",
    "\n",
    "iou_results_df = pd.DataFrame(iou_results)\n",
    "iou_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to csv\n",
    "iou_results_df.to_csv(f\"outputs/{start}_{end}/kpi/iou.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics for the iou results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some summary statistics\n",
    "print(\"Summary by thresholds: \")\n",
    "for threshold in np.arange(0, 1, 0.1):\n",
    "    selector = iou_results_df[\"iou\"] >= threshold\n",
    "    count = len(iou_results_df[selector])\n",
    "\n",
    "    print(f\"predicted obj with iou >= {threshold:.1f}: {count}\")\n",
    "\n",
    "print(\"\\nSummary by occolusion level:\")\n",
    "for oratio in iou_results_df[\"occolusion_ratio\"].unique():\n",
    "    selector = (iou_results_df[\"occolusion_ratio\"] == oratio)\n",
    "    mean = np.mean(iou_results_df[selector][\"iou\"])\n",
    "\n",
    "    print(f\"mean iou for occlusion ratio {oratio}: {mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results by visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bboxes(image_path, pred_bboxes, gt_bboxes):\n",
    "    \"\"\"\n",
    "    Visualize prediction and ground truth bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        pred_bboxes (list of tuples): List of predicted bounding boxes [(x1, y1, x2, y2), ...].\n",
    "        gt_bboxes (list of tuples): List of ground truth bounding boxes [(x1, y1, x2, y2), ...].\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw prediction boxes in blue\n",
    "    for (x1, y1, x2, y2) in pred_bboxes:\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Blue for predictions\n",
    "        cv2.putText(image, 'Pred', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    \n",
    "    # Draw ground truth boxes in green\n",
    "    for (x1, y1, x2, y2) in gt_bboxes:\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green for ground truth\n",
    "        cv2.putText(image, 'GT', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate KPIs (2) - TP, FP, FN, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to calculate TP\n",
    "# TP: prediction = car, truth = Vehicle\n",
    "def cal_kpi(preds: pd.DataFrame, truths: pd.DataFrame):\n",
    "    ''' \n",
    "    This function shall be performed for each image.\n",
    "    The input preds and truths contains the predicted and the ground-truth objects in an image.\n",
    "    '''\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    if len(truths) == 0:\n",
    "        return [0, len(preds), 0, 0, 0, 0, 0]\n",
    "    if_true_detected = np.zeros(len(truths))   # Maintain a list tracking whether a ground-truth object is detected\n",
    "    \n",
    "    # Loop through all predicted objects\n",
    "    for _, pred_obj in preds.iterrows():\n",
    "        # Maintain a list of IoUs\n",
    "        iou_ls = []\n",
    "        \n",
    "        # Calculate IoU with all ground truth bounding boxes\n",
    "        for _, true_obj in truths.iterrows():\n",
    "            iou = calculate_iou(pred_obj[1:5], true_obj[1:5])\n",
    "            iou_ls.append(iou)\n",
    "        \n",
    "        # Find the best match\n",
    "        idx = np.argmax(iou_ls) # This one is the index of the iou maxima and groud truth\n",
    "        iou_max = iou_ls[idx]\n",
    "\n",
    "        # Select threshold\n",
    "        oratio = truths.iloc[idx, -1]\n",
    "        threshold = 0.7 \n",
    "\n",
    "        if oratio == \"Medium\":\n",
    "            threshold = 0.6\n",
    "        \n",
    "        if oratio in [\"Heavy\", \"VeryHeavy\", \"Undefined\"]:\n",
    "            threshold = 0.5\n",
    "\n",
    "        # Compare with the threshold\n",
    "        if iou_max >= threshold:\n",
    "            # Correct detection\n",
    "            if truths[\"class\"].iloc[idx] == \"Vehicle\":\n",
    "                # Correct classification => TP\n",
    "                tp += 1\n",
    "            else:\n",
    "                # Incorrect classification => FP (for the vechile class)\n",
    "                fp += 1\n",
    "            # Update the if_true_detected\n",
    "            if_true_detected[idx] += 1\n",
    "        else:\n",
    "            # Incorrect detection of a vehicle => FP\n",
    "            fp +=1\n",
    "        \n",
    "    # Calculate FN: Vehicles in the ground truth not detected\n",
    "    selector1 = (if_true_detected == 0) # Select objects in the ground truth not detected\n",
    "    obj_not_detected = truths[selector1]\n",
    "    selector2 = (obj_not_detected[\"class\"] == \"Vehicle\") # Select undetected ground truth objects that are vehicle => FN\n",
    "    car_not_detected = obj_not_detected[selector2]\n",
    "    fn = len(car_not_detected)\n",
    "    \n",
    "    # Calculate TN: Non-vehicle objects in the ground truths not detected\n",
    "    selector3 = (obj_not_detected[\"class\"] != \"Vehicle\")\n",
    "    car_not_detected = obj_not_detected[selector3]\n",
    "    tn = len(car_not_detected)\n",
    "\n",
    "    # # Debugging: check the correctness of tp, fp, fn counts\n",
    "    # # Rule 1: tp + fp = the number of predicted car\n",
    "    # print(f\"if tp + fp = the number of predicted car: {tp + fp == len(preds)}\")\n",
    "    # # Rule 2: tp + fn = the number of ground truth car\n",
    "    # print(f\"if tp + fp = the number of ground truth car: {tp + fp == len(truths[truths[\"class\"] == \"Vehicle\"])}\")\n",
    "\n",
    "    # Calculate precision, recall, and accurary\n",
    "    epsilon = 2E-5\n",
    "    precision = round(tp / (tp + fp + epsilon), 3)\n",
    "    recall = round(tp / (tp + fn + epsilon), 3)\n",
    "    accuracy = round((tp + tn)/(tp + tn + fp + fn), 3)\n",
    "    \n",
    "    return [tp, fp, fn, tn, precision, recall, accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all img_ids\n",
    "img_ids = pred_df[\"image_id\"].unique()\n",
    "col_names = [\"image_id\", \"tp\", \"fp\", \"fn\", \"tn\", \"precision\", \"recall\", \"accuracy\"]\n",
    "rows = [] \n",
    "\n",
    "# Iterate through each img\n",
    "for img_id in img_ids:\n",
    "    preds_img = pred_df[pred_df[\"image_id\"]==img_id]\n",
    "    truths_img = true_df[true_df[\"image_id\"]==img_id]\n",
    "    rows.append([img_id] + cal_kpi(preds_img, truths_img))\n",
    "\n",
    "kpi_df = pd.DataFrame(rows, columns=col_names)\n",
    "\n",
    "kpi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "kpi_df.to_csv(f\"outputs/{start}_{end}/kpi/kpi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"precision\\n{kpi_df[\"precision\"].describe()}\\n\")\n",
    "print(f\"recall\\n{kpi_df[\"recall\"].describe()}\\n\")\n",
    "print(f\"accuracy\\n{kpi_df[\"accuracy\"].describe()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_df = []\n",
    "kpi_df = []\n",
    "\n",
    "starts = [1 + 5000 * i for i in range(10)]\n",
    "\n",
    "for start in starts:\n",
    "    end = start + 5000 - 1\n",
    "    iou_df.append(pd.read_csv(f\"outputs/{start}_{end}/kpi/iou.csv\"))\n",
    "    kpi_df.append(pd.read_csv(f\"outputs/{start}_{end}/kpi/kpi.csv\"))\n",
    "\n",
    "iou_df = pd.concat(iou_df, axis=0)\n",
    "kpi_df = pd.concat(kpi_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU summary: \n",
      "count    336282.000000\n",
      "mean          0.685821\n",
      "std           0.230113\n",
      "min           0.000000\n",
      "25%           0.557600\n",
      "50%           0.737150\n",
      "75%           0.876200\n",
      "max           1.000000\n",
      "Name: iou, dtype: float64\n",
      "Precision summary: \n",
      "count    47152.000000\n",
      "mean         0.680207\n",
      "std          0.262376\n",
      "min          0.000000\n",
      "25%          0.500000\n",
      "50%          0.714000\n",
      "75%          0.875000\n",
      "max          1.000000\n",
      "Name: precision, dtype: float64\n",
      "Recall summary: \n",
      "count    47152.000000\n",
      "mean         0.300220\n",
      "std          0.182336\n",
      "min          0.000000\n",
      "25%          0.176000\n",
      "50%          0.281000\n",
      "75%          0.400000\n",
      "max          1.000000\n",
      "Name: recall, dtype: float64\n",
      "\n",
      "Accuracy summary: \n",
      "count    47152.000000\n",
      "mean         0.730965\n",
      "std          0.151093\n",
      "min          0.000000\n",
      "25%          0.638000\n",
      "50%          0.750000\n",
      "75%          0.844000\n",
      "max          1.000000\n",
      "Name: accuracy, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"IoU summary: \\n{iou_df[\"iou\"].describe()}\")\n",
    "print(f\"Precision summary: \\n{kpi_df[\"precision\"].describe()}\")\n",
    "print(f\"Recall summary: \\n{kpi_df[\"recall\"].describe()}\\n\")\n",
    "print(f\"Accuracy summary: \\n{kpi_df[\"accuracy\"].describe()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
